<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./index_files/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>



	
    <!-- Write meta tags so that specified thumnbnail and descriptions appear on pasting the link -->
    <meta property="og:title" content="Discovering and Mitigating Biases in CLIP-based Image Editing">
    <meta property="og:description" content="Research article by Md Mehrab Tanjim, Krishna Kumar Singh, Kushal Kafle, Ritwik Sinha, and Garisson W. Cottrell.">
    <meta property="og:image" content="./resources/teaser.png">
    <meta property="og:url" content="https://mehrab-tanjim.github.io/debiasing_clip_based_image_editing/index.html">
    <title>Text Visualness</title>

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async="" src="https://mehrab-tanjim.github.io/debiasing_clip_based_image_editing/"></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
<script src="chrome-extension://njgehaondchbmjmajphnhlojfnbfokng/js/contentScripts/dom.js"></script></head>

<body data-new-gr-c-s-check-loaded="14.1136.0" data-gr-ext-installed="">
	<br>
	<center>
		<span style="font-size:36px"><b>Discovering and Mitigating Biases in CLIP-based Image Editing</b></span><br>
		<span style="font-size:25px">[<a href="./resources/paper.png"> Paper @ WACV2024 </a>]</span><br><br>
		<span><a href="https://mehrab-tanjim.github.io/">Md Mehrab Tanjim<sup>1</sup></a>, <a href="https://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh<sup>1</sup></a>, <a href="https://research.adobe.com/person/chris-tensmeyer/">Kushal Kafle<sup>1</sup></a>, <a href="https://gujiuxiang.com/">Ritwik Sinha<sup>1</sup></a>, and <a href="https://www.linkedin.com/in/ani-nenkova-6223b9212">Garisson W. Cottrell<sup>2</sup></a></span><br><br>
		<span><sup>1</sup>Adobe Research, <sup>2</sup>UCSD</span><br>
        <br><br>
		<a href="https://research.adobe.com/"><img src="./index_files/adobe-logo.png" width="120px"></a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.cc.gatech.edu/"><img src="./index_files/ucsd.png" width="200px"></a><br><br>


		<span>⭐ Our paper has been accepted at WACV 2024 [<a href="https://mehrab-tanjim.github.io/debiasing_clip_based_image_editing/">Reproduction by others</a>]</span><br><br>
		
	<hr>
	<center>
		<table align="center" width="850px">
			<tbody><tr>
				<td width="260px">
					<center>
						<img class="round" style="width:700px" src="./index_files/teaser.png"><br><br>
						Overview of the sentence visualness identification task, along with a motivating downstream application – image-to-text generation systems like DALL-E can be passively triggered for sentences identified as visual (in this example, the sentence about Tai chi), while skipping generation for non-visual text (the sentence about complaint). 
					</center>
				</td>
			</tr>
		</tbody></table>

		<hr>

	</center>


	<center><h1>Technical Abstract</h1></center><table align="center" width="850px">
		
		<tbody><tr>
			<td>
				Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will unlock the ability to augment text with relevant images, as neural text-to-image generation and retrieval models operate on the implicit assumption that the input text is visual in nature. We curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. Additionally, we use documents that contain text and visual assets to create a distantly supervised corpus of document text and associated images.  We also propose a fine-tuning strategy that adapts large vision-language models like CLIP that assume a one-to-one correspondence between text and image to the task of scoring text visualness from text input alone. Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common <em>NULL</em> image while matching visual text to their corresponding images in the document. We evaluate the proposed approach on its ability to <em>(i)</em> classify visual and non-visual text accurately, and <em>(ii)</em> attend over words that are identified as visual in psycholinguistic studies. Empirical evaluation indicates that our approach performs better than several heuristics and baseline models for the proposed task. Furthermore, to highlight the importance of modeling the visualness of text, we conduct qualitative analyses of text-to-image generation systems like DALL-E. 
			</td>
		</tr>
	</tbody></table>
	<br>
	<hr>
	<center><h1>Paper and Bibtex</h1></center><table align="center" width="1100px">
		
		<tbody><tr>
			<td><a href="https://gaurav22verma.github.io/text-visualness/assets/TextVisualness.pdf"><img class="layered-paper-big" style="height:175px" src="./index_files/screenshot.png"></a></td>
			<td><span style="font-size:14pt">
				<b>Learning the Visualness of Text Using Large Vision-Language Models</b><br>
				Gaurav Verma, Ryan A. Rossi, Christopher Tensmeyer, Jiuxiang Gu, Ani Nenkova<br>
				<em>In Proceedings</em> of The 2023 Conference on Empirical Methods in Natural Language Processing. 2023.<br><br>
				webpage: <a href="https://gaurav22verma.github.io/text-visualness/">https://gaurav22verma.github.io/text-visualness/</a><br>
				arXiv: <a href="https://arxiv.org/abs/2305.10434">https://arxiv.org/abs/2305.10434</a><br><br><br>
			</span></td>
		</tr>
	</tbody></table>
	<br>

	<table align="center" width="670px">
		<tbody><tr>
			<td><span style="font-size:11pt">
				<span style="font-size: 14pt">Bibtex:</span><br><br>
				<left>
				<code>
					<text style="color: red">@inproceedings</text>{verma2023textvisualness,<br>
  					<text style="color: blue">title</text>={Learning the Visualness of Text Using Large Vision-Language Models},<br>
  					<text style="color: blue">author</text>={Verma, Gaurav and Rossi, Ryan A and Tensmeyer, Christopher and Gu, Jiuxiang and Nenkova, Ani},<br>
  					<text style="color: blue">booktitle</text>={The 2023 Conference on Empirical Methods in Natural Language Processing},<br>
  					<text style="color: blue">year</text>={2023}<br>
				</code>
			</left>
			</span></td>
		</tr>
	</tbody></table>
	<br>
	<hr>
	<br>

	<table align="center" width="900px">
		<tbody><tr>
			<td width="400px">
				<center>
					The template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</center>
			</td>
		</tr>
	</tbody></table>

<br>


</center></body><div id="__genieContainer" style="all: initial;"><template shadowrootmode="open"><style>@import "chrome-extension://jjfblogammkiefalfpafidabbnamoknm/built/inject.css";</style><div id="rmnGenieWrappingDiv" style="right: 360px; display: none;"><!----> <div data-v-4b8d7254="" class="bbb-loading-dots" style="display: none !important;"><div data-v-4b8d7254="" class="finish-state start-state"></div> <div data-v-4b8d7254="" class="finish-state start-state"></div> <div data-v-4b8d7254="" class="finish-state start-state"></div></div> <div id="component"></div></div></template></div><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>